---
title: "Module 2 - Notes and Exercises"
author: "<Your Name>"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    theme: journal
    highlight: haddock
    code_folding: hide
    fig_caption: yes
---


```{r, code = readLines("setup.R"), cache = FALSE, include=FALSE}
```

```{r}
knitr::opts_chunk$set(
   echo = TRUE
)
```





# Multi-armed bandits {#mod-bandit}

This module consider the k-armed bandit problem which is a sequential decision problem with one state and $k$ actions. The problem is used to illustrate different learning methods used in RL. 

## Learning outcomes 

By the end of this module, you are expected to:

<!-- Define reward -->
<!-- Understand the temporal nature of the bandit problem -->
<!-- Define k-armed bandit -->
<!-- Define action-values -->
<!-- Define action-value estimation methods -->
<!-- Define exploration and exploitation -->
<!-- Select actions greedily using an action-value function -->
<!-- Define online learning -->
<!-- Understand a simple online sample-average action-value estimation method -->
<!-- Define the general online update equation -->
<!-- Understand why we might use a constant stepsize in the case of non-stationarity -->
<!-- Define epsilon-greedy -->
<!-- Compare the short-term benefits of exploitation and the long-term benefits of exploration -->
<!-- Understand optimistic initial values -->
<!-- Describe the benefits of optimistic initial values for early exploration -->
<!-- Explain the criticisms of optimistic initial values -->
<!-- Describe the upper confidence bound action selection method -->
<!-- Define optimism in the face of uncertainty -->


<!-- * Describe what VBA is. -->
<!-- * Setup Excel for VBA. -->
<!-- * Know how the macro recorder works. -->
<!-- * Make your first program. -->
<!-- * Have an overview over what VBA can do. -->
<!-- * Recorded you first macro using the macro recorder -->

<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Textbook readings

For this week, you will need to read Chapter 2 in @Sutton18. Read it before continuing this module.


## The k-armed bandit problem

Multi-armed bandits attempt to find the best option among a collection of alternatives by learning through trial and error. The name derives from “one-armed bandit,” a slang term for a slot machine — which is a perfect analogy for how these algorithms work. 

```{r bandit, echo=FALSE, out.width="100%", fig.align = "center", fig.cap="A 4-armed bandit."}
knitr::include_graphics("img/bandit.png")
```

Imagine you are facing a wall with $k$ slot machines (see Figure \@ref(fig:bandit)), and each one pays out at a different rate. A natural way to figure out how to make the most money (rewards) would be to try each at random for a while (exploration), and start playing the higher paying ones once you have gained some experience (exploitation). That is, from an agent/environment point of view the agent considers a single state at time $t$ and have to choose among $k$ actions given the environment representing the $k$ bandits. Only the rewards from the $k$ bandits are unknown, but the agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. The objective is to find an optimal policy that maximize the total expected reward. Note since the process only have a single state, this is the same as finding an optimal policy $\pi^*(s) = \pi^* = a^*$ that chooses the action with the highest expected reward. Due to uncertainty, there is an exploration vs exploitation dilemma. The agent have one action that seems to be most valuable at a time point, but it is highly likely, at least initially, that there are actions yet to explore that are more valuable.

<!-- For finding the optimal action a learning strategy that balance the exploration vs. exploitation trade-off. -->

<!-- To summarize: -->

<!-- * The agent are faced repeatedly with a choice of $k$ actions. -->
<!-- * After each choice, you receive a reward from a stationary probability distribution. -->
<!-- * Objective is to maximise total reward over some time period, say 100 time steps. -->

<!-- * Each action has an expected or mean reward based on its probability distribution. We shall call thjs the \textit{value} of the action. We do not know these values with certainty. -->
<!-- * Because of this uncertainty, there is always an exploration vs exploitation problem. We always have one action that we deem to be most valuable at any instant, but it is highly likely, at least initially, that there are actions we are yet to explore that are more valuable. -->

Multi-armed bandits can  be used in e.g. [digital advertising](https://research.facebook.com/blog/2021/4/auto-placement-of-ad-campaigns-using-multi-armed-bandits/). Suppose you are an advertiser seeking to optimize which ads ($k$ to choose among) to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time. 

```{r bandit-choose, echo=FALSE, out.width="100%", fig.align = "center", fig.cap="Which ad to choose?"}
knitr::include_graphics("img/bandit-choose.png")
```

It is reasonable to assume that each of these ads will have different effects, and some will be more engaging than others. That is, each ad has some theoretical — but unknown — click-through-rate (CTR) that is assumed to not change over time. How do we go about solving which ad we should choose (see Figure \@ref(fig:bandit-choose))?


## Estimating the value of an action

How can the value of an action be estimated, i.e. the expected reward of an action $q_*(a) = \mathbb{E}[R_t | A_t = a]$. Assume that at time $t$ action $a$ has been chosen $N_t(a)$ times. Then the estimated action value is
\begin{equation} 
	Q_t(a) = \frac{R_1+R_2+\cdots+R_{N_t(a)}}{N_t(a)},
\end{equation}
Storing $Q_t(a)$ this way is cumbersome since memory and computation requirements grow over time. Instead an *incremental* approach is better. If we assume that $N_t(a) = n-1$ and set $Q_t(a) = Q_n$ then $Q_{n+1}$ becomes:
\begin{align}
  Q_{n+1} &= \frac{1}{n}\sum_{i=1}^{n}R_i \nonumber \\
		    &= \frac{1}{n}\left( R_{n} + \sum_{i=1}^{n-1} R_i \right) \nonumber \\
		    &= \frac{1}{n}\left( R_{n} + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i \right) \nonumber \\
		    &= \frac{1}{n}\left( R_{n} + (n-1)Q_n \right) \nonumber \\
	       &= Q_n + \frac{1}{n} \left[R_n - Q_n\right].
\end{align}
That is, we can update the estimate of the value of $a$ using the previous estimate, the observed reward and how many times the action has occurred ($n$). 

A greedy approach for selecting the next action is
\begin{equation}
A_t =\arg \max_a Q_t(a).
\end{equation}
Here $\arg\max_a$ means the value of $a$ for which $Q_t(a)$ is maximised. A pure greedy approach do not explore other actions. Instead an $\varepsilon$-greedy ppproach is used in which with probability $\varepsilon$ we take a random draw from all of the actions (choosing each action with equal probability) and hereby providing some exploration.

<!-- \begin{itemize} -->
<!-- 	\item Simplest action selection rule is to select the action with the highest estimated value. -->
<!-- 	\item \(\epsilon\)-greedy methods are where the agent selects the greedy option most of the time, and selects a random action with probability \(\epsilon\). -->
<!-- 	\item Three algorithms are tried: one with \(e\)=0 (pure greedy), one with \(e\)=0.01 and another with \(e\)=0.1 -->
<!-- 	\item Greedy method gets stuck performing sub-optimal actions. -->
<!-- 	\item \(e\)=0.1 explores more and usually finds the optimal action earlier, but never selects it more that 91\% of the time. -->
<!-- 	\item \(e\)=0.01 method improves more slowly, but eventually performs better than the e=0.1 method on both performance measures. -->
<!-- 	\item It is possible to reduce \(e\) over time to try to get the best of both high and low values. -->
<!-- \end{itemize} -->

<!-- Note this is the general formula  -->

<!-- \begin{equation} -->
<!-- NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate \right] -->
<!-- \end{equation} -->


Let us try to implement the algorithm using an R6 agent and environment class. First we define the agent that do actions based on an $\epsilon$-greedy strategy, stores the estimated $Q$ values and the number of times an action has been chosen:

```{r, echo=TRUE}
#' R6 Class representing the RL agent
RLAgent <- R6Class("RLAgent",
   public = list(
      #' @field qV Q estimates.
      qV = NULL,  
      
      #' @field nV Action counter.
      nV = NULL,  
      
      #' @field k Number of bandits.
      k = NULL,   
      
      #' @field epsilon Epsilon used in epsilon greed action selection.
      epsilon = NULL, 
      
      #' @description Create an object (when call new).
      #' @param k Number of bandits.
      #' @param epsilon Epsilon used in epsilon greed action selection.
      #' @return The new object.
      initialize = function(k = 10, epsilon = 0.01, ini =  0) {
         self$epsilon <- epsilon
         self$qV <- rep(ini, k)
         self$nV <- rep(0, k)
         self$k <- k
      },
      
      #' @description Clear learning.
      #' @param eps Epsilon.
      #' @return Action (index).
      clearLearning = function() {
         self$qV <- 0
         self$nV <- 0
      },
      
      #' @description Select next action using an epsilon greedy strategy.
      #' @return Action (index).
      selectActionEG = function() {   
         if (runif(1) <= self$epsilon) { # explore
            a <- sample(1:self$k, 1)
         } else { # exploit
            a <- which(self$qV == max(self$qV))
            a <- a[sample(length(a), 1)]
         }
         return(a)
      },
      
      #' @description Update learning values (including action counter).
      #' @param a Action.
      #' @param r Reward.
      #' @return NULL (invisible)
      updateQ = function(a, r) {
         self$nV[a] <- self$nV[a] + 1
         self$qV[a] <- self$qV[a] + 1/self$nV[a] * (r - self$qV[a])
         return(invisible(NULL))
      }
   )
)
```

Next, the environment generating rewards. The true mean reward $q_*(a)$ of an action were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. The observed reward was then generated using a normal distribution with mean $q_*(a)$ and variance 1:

```{r, echo=TRUE}
#' R6 Class representing the RL environment
#' 
#' Assume that bandits are normal distributed with a mean and std.dev of one. 
RLEnvironment <- R6Class("RLEnvironment",
   public = list(
      #' @field mV Mean values
      mV = NULL,  
      
      #' @field k Number of bandits.
      k = NULL,   
      
      #' @description Create an object (when call new).
      #' @param k Number of bandits.
      #' @return The new object.
      initialize = function(k = 10) {
         self$mV <- rnorm(k)
      },
      
      #' @description Sample reward of a bandit.
      #' @param a Bandit (index).
      #' @return The reward.
      reward = function(a) {
         return(rnorm(1, self$mV[a]))
      },
      
      #' @description Returns action with best mean.
      optimalAction = function() return(which.max(self$mV))
   )
)
```

To test the RL algorithm we use a function returning two plots that compare the performance:

```{r}
#' Performance of the bandit algorithm using different epsilons.
#'
#' @param k Bandits.
#' @param steps Time steps.
#' @param runs Number of runs with a new environment generated.
#' @param epsilons Epsilons to be tested.
#' @param ini Initial value estimates.
#' @return Two plots in a list.
performance <- function(k = 10, steps = 1000, runs = 500, epsilons = c(0, 0.01, 0.1), ini = 0) {
   rew <- matrix(0, nrow = steps, ncol = length(epsilons))
   best <- matrix(0, nrow = steps, ncol = length(epsilons))
   for (run in 1:runs) {
      env <- RLEnvironment$new(k)
      oA <- env$optimalAction()
      # print(oA); print(env$mV)
      for (i in 1:length(epsilons)) {
         agent <- RLAgent$new(k, epsilons[i], ini)
         for (t in 1:steps) {
            a <- agent$selectActionEG()
            r <- env$reward(a)
            agent$updateQ(a, r)
            rew[t, i] <- rew[t, i] + r  # sum of rewards generated at t
            best[t, i] <- best[t, i] + (a == oA)
         }
      }
   }
   # rew <- apply(rew, 2, cumsum)/runs   # avg cumsums of each col 
   best <- apply(best, 2, cumsum)/runs
   rew <- rew/runs   # avg of each col 
   # best <- best/runs  
   colnames(rew) <- epsilons
   colnames(best) <- epsilons
   dat1 <- tibble(t = 1:steps) %>%
      bind_cols(rew) %>%   # bind data together
      pivot_longer(!t, values_to = "reward", names_to = "epsilon") %>%   # move rewards to a single column
      group_by(epsilon) %>% 
      mutate(rewAvg = cumsum(reward)/t)  %>% # calc reward
      # mutate(reward = reward/t)  # calc average reward
      mutate(rewMA = rollapply(reward, 50, mean, align = "right", fill = NA))
   dat2 <- tibble(t = 1:steps) %>%
      bind_cols(best) %>%   # bind data together
      pivot_longer(!t, values_to = "optimal", names_to = "epsilon") %>%   # move rewards to a single column
      mutate(optimal = optimal/t)  # calc average
   pt1 <- dat1 %>% 
      ggplot(aes(x = t, y = rewAvg, col = epsilon)) +
      geom_line() +
      geom_line(aes(x = t, y = rewMA, col = epsilon), linetype = "dotted") +
      labs(y = "Average reward per time unit", x = "Time", title = str_c("Average over ", runs, " runs"))
   pt2 <- dat2 %>% 
      ggplot(aes(x = t, y = optimal, col = epsilon)) +
      geom_line() +
      labs(y = "Average number of times optimal action chosen", x = "Time", title = str_c("Average over ", runs, " runs"))
   return(list(ptR = pt1, ptO = pt2))
}
```

We test the performance using 2000 runs over 1000 time steps.

```{r, cache=TRUE, echo=TRUE}
pts <- performance(runs = 2000, steps = 1000)
pts$ptR
pts$ptO
```


## The role of the step-size



## Optimistic initial values












## Exercises {#sec-bandit-ex}



### Exercise - Self-Play 



Consider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above
played against itself. What do you think would happen in this case? Would it learn a different way of playing?






```{r links, child="links.md", include=FALSE}
```
