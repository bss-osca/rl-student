---
title: "Module 3 - Notes and Exercises"
author: "<Your Name>"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    theme: journal
    highlight: haddock
    code_folding: hide
    fig_caption: yes
---


```{r, code = readLines("setup.R"), cache = FALSE, include=FALSE}
```

```{r}
knitr::opts_chunk$set(
   echo = TRUE
)
```


# Markov decision processes {#mod-mdp}

This module consider the k-armed bandit problem which is a sequential decision problem with one state and $k$ actions. The problem is used to illustrate different learning methods used in RL. 

## Learning outcomes 

By the end of this module, you are expected to:



<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Textbook readings

For this week, you will need to read Chapter 2 - 2.7 in @Sutton18. Read it before continuing this module.


## The k-armed bandit problem

## Exercises {#sec-mdp-ex}



### Exercise - 



```{r links, child="links.md", include=FALSE}
```
