---
title: "Module 5 - Notes and Exercises"
author: "<Your Name>"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    theme: journal
    highlight: haddock
    code_folding: hide
    fig_caption: yes
---


```{r, code = readLines("setup.R"), cache = FALSE, include=FALSE}
```

```{r}
knitr::opts_chunk$set(
   echo = TRUE
)
```


# Dynamic programming {#mod-dp}

The term *Dynamic Programming* (*DP*) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the *principle of optimality*. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations. 

DP do both *policy evaluation* (prediction) and *control*. Policy evaluation give us the value function $v_\pi$ given a policy $\pi$. Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations.

Two main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions.

Note the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a "program". 


## Learning outcomes 

By the end of this module, you are expected to:

* Describe the distinction between policy evaluation and control.

<!-- Explain the setting in which dynamic programming can be applied, as well as its limitations -->
<!-- Outline the iterative policy evaluation algorithm for estimating state values under a given policy -->
<!-- Apply iterative policy evaluation to compute value functions -->
<!-- Understand the policy improvement theorem -->
<!-- Use a value function for a policy to produce a better policy for a given MDP -->
<!-- Outline the policy iteration algorithm for finding the optimal policy -->
<!-- Understand “the dance of policy and value” -->
<!-- Apply policy iteration to compute optimal policies and optimal value functions -->
<!-- Understand the framework of generalized policy iteration -->
<!-- Outline value iteration, an important example of generalized policy iteration -->
<!-- Understand the distinction between synchronous and asynchronous dynamic programming methods -->
<!-- Describe brute force search as an alternative method for searching for an optimal policy -->
<!-- Describe Monte Carlo as an alternative method for learning a value function -->
<!-- Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy -->

<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Textbook readings

For this week, you will need to read Chapter 4-4.7 in @Sutton18. Read it before continuing this module. A summary of the book notation can be seen [here](sutton-notation).


## Policy evaluation

The state-value function can be represented using the Bellman equation \@ref(eq:bell-state):
\begin{equation}
v_\pi(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_\pi(s')\right).            
\end{equation}

If the dynamics are known perfectly, this becomes a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns $v_\pi(s), s \in \mathcal{S}$. This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations $v_0, v_1, v_2, \ldots$, with initial approximation $v_0$ chosen arbitrarily e.g. $v_k(s) = 0 \:  \forall s$ (ensuring terminal state = 0). We can update it using the Bellman equation using (*a sweep*):

\begin{equation}
v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_k(s')\right) 
\end{equation}

We call this update an \textit{expected update} because it is based on the expectation over all possible next states, rather than a sample of reward/value from the next state. Eventually this update will converge when $v_k = v_\pi$ after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold $\theta$ (see Figure \@ref(fig:policy-eval-alg)). Note the algorithm uses two arrays to maintain the state-value ($v$ and $V$). Alternatively, a single array could be used that update values in place, i.e. $V$ is used insted of $v$. Hence, values are updated faster. 

```{r policy-eval-alg, echo=FALSE, fig.cap="Iterative policy evaluation."}
knitr::include_graphics("img/policy-evalution.png")
```


## Policy Iteration

Now that we have means to evaluate a policy iteratively, we can look into finding an optimal policy. In general, this is composed of two simple steps:

1. Given a policy $\pi$ (initially $\pi_0$), estimate $v_\pi$ via the policy
evaluation algorithm (iterating a fixed number of times or until it stabilizes),
giving you
  $$v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]$$
2. Generate a new, improved policy $\pi' \geq \pi$ by *greedily* picking
  $$\pi' = \text{greedy}(v_\pi)$$
  Then go back to step (1) to evaluate the policy.

Let's try to understand this deeper. To do so, consider a deterministic policy
$\pi(s) = a$. Then what we are doing in the above two steps is the following:

$$\pi'(s) = \argmax_{a \in \mathcal{A}} q_\pi(s, a)$$

i.e. our new policy will, in each state, pick the action that "gives us the most
q". Now, we can set up the following inequality:

$$
q_\pi(s, \pi'(s)) = \max_{a \in \mathcal{A}} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)
$$

which proves that this greedy policy iteration strategy does indeed work, since
the return we get from starting in state $s$, greedily choosing the locally best
action $\argmax_{a \in \mathcal{A}} q(s, a)$ and from thereon following the old
policy $\pi$, must be at least as high as if we had chosen any particular action
$\pi(s)$ and not the optimal one (basically, the maximum of a sequence is at
least as big as any particular value of the sequence).

(Note: I assume the equality in $q_\pi(s, \pi(s)) = v_\pi(s)$ is meant in
expectation?)

What we can show now is that using this greedy strategy not only improves the
next step, but the entire value function. For this, we simply need to do some
expansions inside our definition of the state-value function as a Bellman
expectation equation:

$$
\begin{align}
  v_\pi(s) &\leq q_\pi(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma (R_{t+2} + \gamma^2 v_\pi(S_{t+2})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, \pi'(S_{t+2})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...)) | S_t = s] \\
&= v_{\pi'}(s)
\end{align}
$$

So in total, we have $v_\pi(s) \leq v_{\pi'}(s)$. Furthermore, if at one point
the policy iteration stabilizes and we have equality in the previous equation

$$
q_\pi(s, \pi'(s)) = \max_{a \in \mathcal{A}} q_\pi(s, a) = q_\pi(s, \pi(s)) = v_\pi(s)
$$

then we also have

$$\max_{a \in \mathcal{A}} q_\pi(s, a) = v_\pi(s)$$

which is precisely the Bellman optimality equation. So at this point, it holds that

$$v_\pi(s) = v_\star(s)\, \forall s \in \mathbb{S}.$$

The last question we must answer is how many steps of policy iteration we should
do to find the optimal policy? Definitely not infinitely many, since we often
notice that the value function stabilizes quite rapidly at some point. So there
are two basic methods:

1. Use $\varepsilon$-convergence, meaning we stop when all values change less than some amount $\varepsilon$ or
2. Just use a fixed number of steps $k$ (thereby introducing another hyperparameter).

## Value Iteration

The next dynamic programming method we want to consider is *value iteration*. In
this case, it is not directly our aim to improve the policy, but rather aims
directly at improving the value function (policy iteration does this as well,
but as a side effect). Basically, while policy iteration iterated on the Bellman
expectation equation, value iteration now iterates on the Bellman *optimality*
equation via the following update rule:

$$
v_\star(s) \gets \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\star(s')
$$

or, for the step $k \rightarrow k + 1$:

$$
v_{k+1} \gets \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_k(s')
$$

Notice how we assume we already know the solutions to the "subproblems", i.e.
$v_\star(s')$ and then work backwards to find the best solution to the actual
problem (essence of dynamic programming). As such, practically, we can begin
with some initial estimate of the target state-value and then iteratively update
the previous state-values.

Note how value-iteration effectively combines one sweep of policy evaluation,
i.e. one "backup", with one step of policy iteration (improvement), since it
performs a greedy update while also evaluating the current policy. Also, it is
important to understand that the value-iteration algorithm does not require a
policy to work. No actions have to be chosen. Rather, the q-values (rewards +
values of next states) are evaluated to update the state-values. In fact, the
last step of value-iteration is to *output* the optimal policy $\pi^\star$:

$$
\pi^\star(s) = \argmax_a q(s, a) = \argmax_a R_s^a + \gamma\sum_{s' \in \mathcal{S}} P_{ss'}^a v_{\pi^\star}(s')
$$

To derive the above equation, remind yourself of the Bellman optimality equation
for the state-value function:

$$v_\star(s) = \max_a q_\star(s, a)$$

and that for the action-value function:

$$q_\star(s, a) = \mathcal{R}_s^a + \gamma\sum_{s \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\star(s)$$

and plug the latter into the former. Basically, at each time step we will update
the value function for a particular state to be the maximum q value.

## Summary of DP Algorithms (so far)

At this point, we know three DP algorithms that solve two different problems. We
know the *policy evaluation*, *policy iteration* and *value iteration*
algorithms that solve the prediction (1) and control (2, 3) problems,
respectively. Let's briefly summarize each:

1. The goal of __policy evaluation__ is to determine $v_\pi$ given $\pi$. It does so
by starting with some crude estimate $v(s)$ and iteratively evaluating $\pi$.
$v(s)$ is updated to finally give $v_\pi$ via the *Bellman expectation
equation*:
$$
\begin{align}
  v_{k+1} &= \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s, a) \\
  &= \sum_{a \in \mathcal{A}}\pi(a | s)\left(R_s^a + \gamma\sum_{s \in \mathcal{S}} P_{ss'}^a v_k(s')\right)
\end{align}
$$

2. __Policy iteration__ combines policy evaluation and the Bellman expectation
equation with updates to the policy. Instead of just updating the state values,
we also update the policy, setting the chosen action in each state to the one
with the highest q-value:
    1. Policy Evaluation,
    2. $\pi' = \text{greedy}(v_\pi)$
Policy iteration algorithms have a time-complexity of $O(mn^2)$ for $m$ actions and $n$ states.
3. Lastly, __Value iteration__ uses the Bellman *optimality* equation to
iteratively update the value of each state to the maximum action-value
attainable from that state
$$
v_{k+1} \gets \max_{a \in \mathcal{A}} q_\pi(s, a) = \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_k(s')
$$
and finally also outputs the optimal policy: $\pi^\star(s) = \argmax_a q(s, a)$
Value iteration algorithms have a time-complexity of $O(m^2n^2)$ for $m$ actions and $n$ states.

## Extensions

One extension to dynamic programming as we have discussed it above is
*asynchronous* DP, where states are updated individually, in any order. This can
significantly improve computation.

The first way to achieve more asynchronous DP is to use *in-place DP*.
Basically, instead of keeping a copy of the old and new value function in each
value-iteration update, you can just update the value functions in-place. The
thing to note here is that asynchronous updates in other parts of the
state-space will directly be affected by this. However, the point is that this
is not actually a bad idea.

An extension of this is *prioritized sweeping*. The idea here is to keep track
of how "effective" or "significant" updates to our state-values are. States
where the updates are more significant are likely further away from converging
to the optimal value. As such, we'd like to update them first. For this, we
would compute this significance, called the *Bellman error*:

$$|v_{k+1}(s) - v_k(s)|$$

and keep these values in a priority queue. You can then efficiently pop the top
of it to always get the state you should update next.

An additional improvement is to do *prioritize local updates*. The idea is that
if your robot is in a particular region of the grid, it is much more important
to update nearby states than faraway ones.

Lastly, in very high dimensional spaces and problems with high branching factor,
it makes sense to sample actions (branches).


## Summary 

Read Chapter 4.8 in @Sutton18.

## Exercises



### Exercise - Gambler's problem

Consider the gambler's problem in Exercise \@ref(mdp-1-ex-gambler).

   1) Solve the problem using ...


### Exercise - Car rental

Consider the car rental problem in Exercise \@ref(mdp-2-ex-car) with inventory dynamics: 
$$X = \min(20, \max(0, x' - a - D_1) + H_1))),$$ 
and 
$$Y = \min(20, \max(0, y' + a - D_2) + H_2))),$$
for Location 1 and 2, respectively. The transition probabilities can be split due to independence: $$ p((x,y) | (x',y'), a) = p(x | x', a) p(y | y', a) $$

<!-- $$ p((x,y) | (x',y'), a) = p(x | x', a) p(y | y', a) = \Pr(x = x' + n_x - a)\Pr(y = y' + n_y + a) = \Pr(n_x = x - x' + a)\Pr(n_y = y - y' - a) = \phi(x - x' + a)\phi(y - y' - a) $$ -->
<!-- For location 1: -->

<!-- $$ -->
<!-- p(x | x', a) = \Pr(x = \min(20, \max(0, x' - a - D_1) + H_1))) = -->
<!-- \begin{cases} -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 = x) & x < 20\\ -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 \geq 20) & x = 20 -->
<!-- \end{cases} -->
<!-- $$ -->

   1) Solve the problem using ...




```{r links, child="links.md", include=FALSE}
```
